RUS GPT
https://ai.wendabeta.net/?utm_source=sharenav#/chat/1718426451611
ALL GPT
https://github.com/LiLittleCat/awesome-free-chatgpt/blob/main/README_en.md

CTF TOOLS TO DOWNLOAD
https://github.com/TeamNameBE/CTF-Tools?tab=readme-ov-file
CTF TOOLS
https://github.com/gregalletti/CTF_tools?tab=readme-ov-file
Awesome CTF Cheatsheet  (baza bazi komand na ctf)
https://github.com/uppusaikiran/awesome-ctf-cheatsheet?tab=readme-ov-file#forensics
Stego helper (chtob ne obosratsya na stege)
https://0xrick.github.io/lists/stego/#tools


https://www.cryptool.org/en/ct1/downloads/

https://docs.google.com/document/d/1QuFu8e4nVbEDNOb9nVABX5apukDFcgGtGnDCLKxqh30/edit#heading=h.i7gbk4evid8z

чтоб не кринжануть на шлюзах ...
https://moonback.ru/page/debian-11-static-routes?ysclid=lxhwnj5ca957936287
бдмарии
https://blog.sedicomm.com/2019/11/18/12-rekomendatsij-po-bezopasnosti-v-mysql-mariadb-dlya-linux/
https://www.8host.com/blog/zashhita-mysql-i-mariadb-na-servere-linux/?ysclid=lxhwoz5umr322577052


https://docs.google.com/document/d/16tyFVnW12N4lgAD_HqpVnvCHruAKe-z2Nl9J_FsjnUg/edit
https://docs.google.com/document/d/18DFR1fKpMqrrbg6CLw-FkEtviXAWwQd5yZ1yUEIZffw/edit
1. шлюзы - почтарь чекннуть - локальный репо починить 
/etc/apt/sources.list.d/repos.list:
deb https://security.debian.org/debian-security/ buster/updates main contrib non-free


https://cybersecuritynews.com/penetration-testing-tools/
https://github.com/am0nt31r0/Penetration-Testing-Mind-Map/blob/master/pentest_methodology_and_tools.png
https://habr.com/ru/companies/cloud4y/articles/651831/



https://www.hackthebox.com/blog/memory-forensics-volatility-write-up#mcetoc_1gr5nhntg9ds
https://blog.onfvp.com/post/volatility-cheatsheet/
https://cheatography.com/bpdzone/cheat-sheets/volatility-3-0-windows/
https://github.com/cyb3rmik3/DFIR-Notes?tab=readme-ov-file
https://book.hacktricks.xyz/generic-methodologies-and-resources/basic-forensic-methodology/memory-dump-analysis/volatility-cheatsheet

1-3
Целевая переменная purchase
Принимаем уровень отсечки 0.35, так как при увеличение или уменьшении значения отсечки качество модели ухудшается согласно матрице ошибок модель начинает чаще предсказывать 0 вместо 1. 
ROC – кривая позволяет оценить качество бинарной классификации. Чем больше кривая ROC охватывает верхний левый угол графика, тем лучше модель классифицирует данные по категориям
По графику ROC-кривой видим, что модель работает средне. При построении графика метрик модели видим, что все графики принимают близкие значения на интервале от 0.3 до 0.4. Поэтому выбираем точку отсечки 0.35
3-6
1. Целевая переменная - Данные. H0: cтатистически значимой связи нет между целевой переменной и  признаками. H1: связь есть."							
2. Корреляционная матрица на листе "Корреляция признаков". Вывод: мультиколлинеарности между объясняющими переменными нет, так значения корреляции меньше 0.7														
Сильная связь между waistline и weight; LDL_chole и tot_chole							
Между waistline и weight выбираем weight, так как у него более сильная связь с целевой переменной							
Между LDL_chole и tot_chole выбираем tot_chole, так как у него более сильная связь с целевой переменной	
5. Расчет частных коэффициентов на листе "Регрессия"							
"6. Теснота связи между признаками.  Для измерения степени тесноты связи между изменениями величины результативного признака (у) и изменениями значений факторных признаков определяется коэффициент множественной (совокупной) корреляции (R).						"										
Вывод: Fрасч > Fтабл, значит с вероятностью 0,95 можно утверждать, что уравнение регрессии статистически значимое и гипотеза H0 отвергается в пользу гипотезы H1							
Fрасч = F
Fтабл = =FРАСПОБР(0,05;16(регрессия)-1;405-16-1)
Р-значение должно быть меньше 0.05 (уровня значимости)	
Смотрим знаки p-значения и t-статистики	
Знаки нижние 95% и верхние 95% должны быть одинаковые	
Частные коэффициенты показывают, как изменение факторных признаков влияет на изменение целевой функции				
На листе "Регрессия" был произведен отбор признаков на основе анализа p-значений, t-статистики, нижние и верхние 95%. Исключены признаки weight, sight_left, sight_right, DBP, BLDS, tot_chole, HDL_chole, triglyceride, SGOT_AST, SGOT_ALT, gamma_GTP. Признаки, которые включены в модель на листе "Отобранные признаки".							
					
							
							
			
В модель включены именно эти признаки, поскольку они не мультиколлинеарны, связь между признаками и целевой переменной существенная				
				
				
	
Выделенные желтым признаки исключаем	
							
Множественный R	0,859567098	Вывод: теснота связи высокая, так как находится в промежутке [0,7; 0,9]					
							
							
							




Целевая переменная got drunk. Принимаем уровень отсечки 0,6, так как при увеличение или уменьшении значения отсечки качество модели ухудшается, в том числе accuracy. ROC – кривая позволяет оценить качество бинарной классификации. Чем больше кривая ROC охватывает верхний левый угол графика, тем лучше модель классифицирует данные по категориям. По графику ROC-кривой видим, что модель работает средне. При построении графика метрик модели видим, что все графики принимают близкие значения на интервале от 0.55 до 0.6. Поэтому выбираем точку отсечки 0.6
Из содержательных соображений значения, где возраст меньше 18 лет не войдут в обучающую выборку	
Прогнозируемая переменная (target) - got drunk	
В качестве предикторов взяты все переменные в датасете	
Пропущенных значений нет	

1	Что подразумевают под функциональной и статистической зависимостью? Назовите частные случаи статистичесокй зависимости.
Функциональная зависимость – это связь, при которой каждому значению независимой переменной x соответствует точно определенE ное значение y. В экономических процессах такой вид зависимости между переменными встречается редко. Для этих процессов характерно взаимодействие случайных факторов. Существующая зависимость между признаками может проявляться не в каждом отдельном случае, а лишь «в общем и среднем» при большом количестве наблюдений. 
2) Статистическая зависимость – это связь, при которой каждому значению независимой переменной x соответствует множество значений зависимой переменной y, причем заранее не известно, какое именно значение примет y. Частным случаем статистической зависиE мости является корреляционная зависимость. 
Корреляционная зависимость – это связь, при которой каждому значению независимой переменной x соответствует определенное средE нее значение (математическое ожидание) зависимой переменной y. 
В регрессионном анализе рассматриваются односторонние зависиE мости случайной переменной Y от неслучайной независимой переменE ной X. Такая зависимость может возникнуть, когда при каждом фиксированном значении X соответствующие значения Y подвержены случайному разбросу за счет действия ряда неконтролируемых фактоE ров. Тогда зависимость между X и Y, представленную в виде соотноE шения Y f(X) называют модельным уравнением регрессии (или просто уравнением регрессии).
Функциональная и корреляционная связь в зависимости от направE ления действия бывает прямая и обратная. По аналитическому выражеE нию зависимость может быть прямолинейной (линейной) и криволиE нейной (нелинейной). В зависимости от количества признаков, включенных в модель, корреляционные связи делят на однофакторные (парные, Y f(X)) и многофакторные (множественные, Y f X1,X2,...,Xn 
2	Что же такое Data Mining? Что обусловило  его развитие?

Data Mining - мультидисциплинарная область, возникшая и развивающаяся на базе таких наук как прикладная статистика, распознавание образов, искусственный интеллект, теория баз данных и др., см. рис. 1.1.
Data Mining - это процесс поддержки принятия решений, основанный на поиске в данных скрытых закономерностей (шаблонов информации) [3].
Технологию Data Mining достаточно точно определяет Григорий Пиатецкий-Шапиро (Gregory Piatetsky-Shapiro) - один из основателей этого направления:
Data Mining - это процесс обнаружения в сырых данных ранее неизвестных, нетривиальных, практически полезных и доступных интерпретации знаний, необходимых для принятия решений в различных сферах человеческой деятельности.

Возникновение и развитие Data Mining обусловлено различными факторами, основные среди которых являются следующие [2]:
•	совершенствование аппаратного и программного обеспечения;
•	совершенствование технологий хранения и записи данных;
•	накопление большого количества ретроспективных данных;
•	совершенствование алгоритмов обработки информации.
3	Как можно охарактеризовать суть и цель технологии Data Mining 
Суть и цель технологии Data Mining можно охарактеризовать так: это технология, которая предназначена для поиска в больших объемах данных неочевидных, объективных и полезных на практике закономерностей.
Неочевидных - это значит, что найденные закономерности не обнаруживаются стандартными методами обработки информации или экспертным путем.
Объективных - это значит, что обнаруженные закономерности будут полностью соответствовать действительности, в отличие от экспертного мнения, которое всегда является субъективным.
Практически полезных - это значит, что выводы имеют конкретное значение, которому можно найти практическое применение.
Знания - совокупность сведений, которая образует целостное описание, соответствующее некоторому уровню осведомленности об описываемом вопросе, предмете, проблеме и т.д.
Использование знаний (knowledge deployment) означает действительное применение найденных знаний для достижения конкретных преимуществ (например, в конкурентной борьбе за рынок).

4	Понятие Business Intelligence. Состав рынка  BI

Business Intelligence - программные средства, функционирующие в рамках предприятия и обеспечивающие функции доступа и анализа информации, которая находится в хранилище данных, а также обеспечивающие принятие правильных и обоснованных управленческих решений.
Понятие BI объединяет в себе различные средства и технологии анализа и обработки данных масштаба предприятия.
На основе этих средств создаются BI-системы, цель которых - повысить качество информации для принятия управленческих решений.
BI-системы также известны под названием Систем Поддержки Принятия Решений (СППР, DSS, Decision Support System). Эти системы превращают данные в информацию, на основе которой можно принимать решения, т.е. поддерживающую принятие решений.
Gartner Group определяет состав рынка систем Business Intelligence как набор  программных продуктов следующих классов:
•	средства построения хранилищ данных (data warehousing, ХД);
•	системы оперативной аналитической обработки (OLAP);
•	информационно-аналитические системы (Enterprise Information Systems, EIS);
•	средства интеллектуального анализа данных (data mining);
•	инструменты для выполнения запросов и построения отчетов (query and reporting tools).
Классификация Gartner базируется на методе функциональных задач, где программные продукты каждого класса выполняют определенный набор функций или операций с использованием специальных технологий.

5	Отличия Data Mining от других методов анализа данных
Традиционные методы анализа данных (статистические методы) и OLAP в основном ориентированы на проверку заранее сформулированных гипотез (verification-driven data mining) и на "грубый" разведочный анализ, составляющий основу оперативной аналитической обработки данных (OnLine Analytical Processing, OLAP), в то время как одно из основных положений Data Mining - поиск неочевидных закономерностей.
Инструменты Data Mining могут находить такие закономерности самостоятельно и также самостоятельно строить гипотезы о взаимосвязях. Поскольку именно формулировка гипотезы относительно зависимостей является самой сложной задачей, преимущество Data Mining по сравнению с другими методами анализа является очевидным.
Большинство статистических методов для выявления взаимосвязей в данных используют концепцию усреднения по выборке, приводящую к операциям над несуществующими величинами, тогда как Data Mining оперирует реальными значениями.
OLAP больше подходит для понимания ретроспективных данных, Data Mining опирается на ретроспективные данные для получения ответов на вопросы о будущем.
6	Задачи Data Mining. Виды классификации 
- системное распределение изучаемых предметов, явлений, процессов по родам, видам, типам, по каким-либо существенным признакам для удобства их исследования; группировка исходных понятий и расположение их в определенном порядке, отражающем степень этого сходства.

Классификация - упорядоченное по некоторому принципу множество объектов, которые имеют сходные классификационные признаки (одно или несколько свойств), выбранных для определения сходства или различия между этими объектами.

Классификация требует соблюдения следующих правил:
	в каждом акте деления необходимо применять только одно основание;
	деление должно быть соразмерным, т.е. общий объем видовых понятий должен равняться объему делимого родового понятия;
	члены деления должны взаимно исключать друг друга, их объемы не должны перекрещиваться;
	деление должно быть последовательным.

Различают:

	вспомогательную (искусственную) классификацию, которая производится по внешнему признаку и служит для придания множеству предметов (процессов, явлений) нужного порядка;
	естественную классификацию, которая производится по существенным признакам, характеризующим внутреннюю общность предметов и явлений. Она является результатом и важным средством научного исследования, т.к. предполагает и закрепляет результаты изучения закономерностей классифицируемых объектов.

В зависимости от выбранных признаков, их сочетания и процедуры деления понятий классификация может быть:

	простой - деление родового понятия только по признаку и только один раз до раскрытия всех видов. Примером такой классификации является дихотомия, при которой членами деления бывают только два понятия, каждое из которых является противоречащим другому (т.е. соблюдается принцип: "А и не А");
	сложной - применяется для деления одного понятия по разным основаниям и синтеза таких простых делений в единое целое. Примером такой классификации является периодическая система химических элементов.

Под классификацией будем понимать отнесение объектов (наблюдений, событий) к одному из заранее известных классов.

Классификация — это закономерность, позволяющая делать вывод относительно определения характеристик конкретной группы. Таким образом, для проведения классификации должны присутствовать признаки, характеризующие группу, к которой принадлежит то или иное событие или объект (обычно при этом на основании анализа уже классифицированных событий формулируются некие правила).

Классификация относится к стратегии обучения с учителем (supervised learning), которое также именуют контролируемым или управляемым обучением.

Задачей классификации часто называют предсказание категориальной зависимой переменной (т.е. зависимой переменной, являющейся категорией) на основе выборки непрерывных и/или категориальных переменных.

Например, можно предсказать, кто из клиентов фирмы является потенциальным покупателем определенного товара, а кто - нет, кто воспользуется услугой фирмы, а кто - нет, и т.д. Этот тип задач относится к задачам бинарной классификации, в них зависимая переменная может принимать только два значения (например, да или нет, 0 или 1).

Другой вариант классификации возникает, если зависимая переменная может принимать значения из некоторого множества предопределенных классов. Например, когда необходимо предсказать, какую марку автомобиля захочет купить клиент. В этих случаях рассматривается множество классов для зависимой переменной.

Классификация может быть одномерной (по одному признаку) и многомерной (по двум и более признакам).

Многомерная классификация была разработана биологами при решении проблем дискриминации для классифицирования организмов. Одной из первых работ, посвященных этому направлению, считают работу Р. Фишера (1930 г.), в которой организмы разделялись на подвиды в зависимости от результатов измерений их физических параметров. Биология была и остается наиболее востребованной и удобной средой для разработки многомерных методов классификации.

7	Процесс классификации 
состоит из двух этапов [21]: конструирования модели и ее использования.

1.	Конструирование модели: описание множества предопределенных классов.
	Каждый пример набора данных относится к одному предопределенному классу.
	На этом этапе используется обучающее множество, на нем происходит конструирование модели.
	Полученная модель представлена классификационными правилами, деревом решений или математической формулой.
2.	Использование модели: классификация новых или неизвестных значений.
	Оценка правильности (точности) модели.
1.	Известные значения из тестового примера сравниваются с результатами использования полученной модели.
2.	Уровень точности - процент правильно классифицированных примеров в тестовом множестве.
3.	Тестовое множество, т.е. множество, на котором тестируется построенная модель, не должно зависеть от обучающего множества.
	Если точность модели допустима, возможно использование модели для классификации новых примеров, класс которых неизвестен.

8	Методы, применяемые для решения задач классификации

Для классификации используются различные методы. Основные из них:

	классификация с помощью деревьев решений;
	байесовская (наивная) классификация;
	классификация при помощи искусственных нейронных сетей;
	классификация методом опорных векторов;
	статистические методы, в частности, линейная регрессия;
	классификация при помощи метода ближайшего соседа;
	классификация CBR-методом;
	классификация при помощи генетических алгоритмов.

9	Задача кластеризации. Цели, области применения
Задача кластеризации сходна с задачей классификации, является ее логическим продолжением, но ее отличие в том, что классы изучаемого набора данных заранее не предопределены.

Синонимами термина "кластеризация" являются "автоматическая классификация", "обучение без учителя" и "таксономия".

Кластеризация предназначена для разбиения совокупности объектов на однородные группы (кластеры или классы). Если данные выборки представить как точки в признаковом пространстве, то задача кластеризации сводится к определению "сгущений точек".

Цель кластеризации - поиск существующих структур.

Кластеризация является описательной процедурой, она не делает никаких статистических выводов, но дает возможность провести разведочный анализ и изучить "структуру данных".

Само понятие "кластер" определено неоднозначно: в каждом исследовании свои "кластеры". Переводится понятие кластер (cluster) как "скопление", "гроздь".

Кластер можно охарактеризовать как группу объектов, имеющих общие свойства. Характеристиками кластера можно назвать два признака:
•	внутренняя однородность;
•	внешняя изолированность.

Вопрос, задаваемый аналитиками при решении многих задач, состоит в том, как организовать данные в наглядные структуры, т.е. развернуть таксономии.

10	Оценка качества кластеризации

Оценка качества кластеризации может быть проведена на основе следующих процедур:

	ручная проверка;
	установление контрольных точек и проверка на полученных кластерах;
	определение стабильности кластеризации путем добавления в модель новых переменных;
	создание и сравнение кластеров с использованием различных методов. Разные методы кластеризации могут создавать разные кластеры, и это является нормальным явлением. Однако создание схожих кластеров различными методами указывает на правильность кластеризации.

Опишите цикл аналитики больших данных 
можно описать следующим этапом -
•	Определение бизнес-проблемы
•	Research
•	Оценка человеческих ресурсов
•	Получение данных
•	Изменение данных
•	Хранилище данных
•	Исследовательский анализ данных
•	Подготовка данных для моделирования и оценки
•	Modeling
•	Implementation

11	Роль и компетенции специалиста по данным
Основные навыки, которыми должен обладать компетентный аналитик данных, перечислены ниже:
•	Деловое понимание
•	SQL программирование
•	Дизайн и реализация отчета
•	Разработка дашбордов
Роль специалиста по данным обычно связана с такими задачами, как прогнозное моделирование, разработка алгоритмов сегментации, рекомендательные системы, фреймворки A / B-тестирования и часто работа с необработанными неструктурированными данными.
Характер их работы требует глубокого понимания математики, прикладной статистики и программирования. Между аналитиком данных и специалистом по анализу данных есть несколько общих навыков, например, способность запрашивать базы данных. Оба анализируют данные, но решение специалиста по данным может иметь большее влияние на организацию.
Вот набор навыков, которые обычно необходимы специалисту по данным:
•	Программирование в статистическом пакете, таком как: R, Python, SAS, SPSS или Julia
•	Возможность очищать, извлекать и исследовать данные из разных источников
•	Исследование, разработка и внедрение статистических моделей
•	Глубокие статистические, математические и компьютерные знания

12	Сформулируйте свойства метрики. Какие метрики часто используют при кластеризации данных, измеренных в количественной шкале?
Остановимся на метриках и сформулируем свойства метрики.
Пусть i, j, k – некоторые точки, а d(i, j) – метрика, расстояние от точки i до точки j. Тогда:

1. d(i, i) = 0;
2.  d(i, j) ≥ 0;
3. d(i, j) = d(j, i);
4. d(i, k) ≤ d(i, j) + d(j, k).

Переводя эти свойства на менее формальный язык, получим довольно логичные утвержде- ния: расстояние от точки до самой себя равно 0, расстояние не бывает отрицательным, рас- стояние от точки i до точки j – то же самое, что расстояние от точки j до точки i, расстояние от точки i до точки k меньше суммы расстояния от точки i до точки j  и расстояния от точки j до точки k (неравенство треугольника).
Давайте рассмотрим основные виды метрик, которые часто используют при кластеризации данных, измеренных в количественной шкале.
Для определённости давайте зафиксируем, что в p-мерном пространстве у нас есть две точки xi = (xi1, xi2, . . . , xip) и xj = (xj1, xj2, . . . , xjp). Если обозначения кажутся не совсем понятны- ми, посмотрите, как у нас записан массив X в постановке задачи (xi и xj – просто строки, соответствующие i-тому и j-тому наблюдению).
Итак, виды метрик для данных в количественной шкале:
про
1.	 Евклидово расстояние, 
2.	Квадрат евклидова расстояния, также обозначается L2 squared, необходимо для неко- торых методов агломерации, в частности, для метода Уорда (Варда):


3.	Манхэттенское расстояние, оно же блочное расстояние, также обозначается L1:
4.	Чебышева

13	Назовите параметры кластеризации
Кластерный анализ (скорее всего не нужно, по логике имеется ввиду иерархическая кластеризация):
Лекция: Анализ БД_лекция 6
Процесс кластеризации зависит от выбранного метода и почти всегда является итеративным. Он может стать увлекательным процессом и включать множество экспериментов по выбору разнообразных параметров, например, меры расстояния, типа стандартизации переменных, количества кластеров и т.д.
В таблице 5.2 приведено сравнение некоторых параметров задач классификации и кластеризации.
 
Пояснения: 
Контролируемость обучения: Кластеризация не предполагает наличия заранее заданных меток классов.
Стратегия: Метки классов не предоставляются, алгоритмы пытаются определить структуры или группы данных самостоятельно.
Наличие метки класса: Данные не имеют предварительно определённых меток классов, что делает задачу неконтролируемой.
Основание для классификации: Основной целью является выявление групп или кластеров в данных, что помогает понять их внутреннюю структуру.
Иерархический кластерный анализ:
Лекция: Иерархический_кластерный_анализ_lecture09_1.pdf В основе данного вида кластерного анализа лежат два предположения:
1. На самом первом шаге кластерного анализа количество кластеров совпадает с количеством наблюдений (имеем n кластеров, состоящих ровно из одного наблюдения).
2. Количество кластеров заранее неизвестно, мы объединяем точки в кластеры до тех пор, пока не получим один большой кластер. Так, на первом шаге иерархического кластерного анализа у нас есть n кластеров, на втором шаге (n−1) кластеров, на третьем уже (n−2) кластеров, и так далее, а на последнем шаге остаётся один кластер. Другими словами, мы выстраиваем некоторую иерархию из кластеров, вложенных друг в друга, а потом решаем, на каком делении, более детальном (много маленьких кластеров) или более общем (мало больших кластеров), стоит остановиться.
У иерархического кластерного анализа есть два параметра кластеризации:
1. Метрика: мера расстояния между точками (наблюдениями). 
2. Метод агломерации или метод агрегирования: алгоритм, который позволяет решать, каким образом объединять точки в кластеры на основе выбранной метрики (про метрики в 12 вопросе)
14	Назовите основные методы агломерации
Лекция: Иерархический_кластерный_анализ_lecture09_1.pdf Метод агломерации или метод агрегирования: алгоритм, который позволяет решать, каким образом объединять точки в кластеры на основе выбранной метрики.
Можно выделить следующие основные методы агломерации.
1	Метод ближнего соседа, он же метод одиночной связи (single linkage). 
Реализация. Расстояние между двумя кластерами A и B определяется как расстояние между ближайшими точками этих кластеров. Считаем расстояния между всеми парами точек, одна точка в паре из кластера A, вторая – из кластера B, затем выбираем минимальное из посчитанных – это и будет расстояние между кластерами A и B. 
Особенности. Имеет недостаток: склонен образовывать кластеры, состоящие из одного наблюдения (монокластеры). 
2. Метод дальнего соседа, он же метод полной связи (complete linkage). 
Реализация. Расстояние между двумя кластерами A и B определяется как расстояние между дальними точками этих кластеров. Считаем расстояния между всеми парами точек, одна точка в паре из кластера A, вторая – из кластера B, затем выбираем максимальное из посчитанных – это и будет расстояние между кластерами A и B. 
Особенности. Вполне надёжный метод, используется в качестве метода агломерации по умолчанию функцией hclust() в R. 
3. Метод средней связи (average linkage). 
Реализация. Расстояние между двумя кластерами A и B определяется как среднее расстояние между точками этих кластеров. Считаем расстояния между всеми парами точек, одна точка в паре из кластера A, вторая – из кластера B, затем считаем среднее арифметическое – это и будет расстояние между кластерами A и B. 
Особенности. Особых примет нет, тоже вполне надёжный метод. 
4. Метод центроидной связи (centroid linkage). 
Реализация. Расстояние между двумя кластерами A и B определяется как расстояние между центроидами (центрами тяжести) кластеров. Центроид – средний вектор кластера, его координаты считаются как средние арифметические соответствующих переменных6. 
Особенности. Имеет недостаток: может вызывать инверсию–ситуацию, когда на последующем шаге кластеризации объединение в кластеры происходит на расстоянии меньшем, чем на предыдущем шаге. Инверсия противоречит самой логике иерархического кластерного анализа: для минимальной потери информации об исходных наблюдениях (а мы теряем её, переходя к группам), мы должны на каждом шаге кластеризации объединять точки в более крупные кластеры, которые в большей степени удалены друг от друга, а здесь мы «скачем» от большего расстояния к меньшему. 
5. Метод Уорда, он же метод Варда (Ward’s linkage). 
Реализация. На каждом шаге обновления кластеров точка присоединяется к тому кластеру, присоединение к которому приводит к минимально возможному увеличению внутригрупповой дисперсии этого кластера. Внутригрупповую дисперсию можно определить следующим образом:
 
Особенности. Считается одним из самых эффективных методов7 агломерации, требует использования только одной метрики – квадрата евклидова расстояния.
15	Что является визуализатором результатов иерархического кластерного анализа? Выбор числа кластеров.
Лекция: Иерархический_кластерный_анализ_lecture09_1.pdf Запустим кластерный анализ и построим дендрограмму – график, который визуализирует результаты иерархического кластерного анализа и позволяет увидеть все возможные варианты кластеризации, от наиболее детальной, где много маленьких кластеров, до наиболее общей, где мало больших кластеров.
По горизонтальной оси на дендрограмме отмечаются сами наблюдения, по вертикальной – расстояния между наблюдениями или кластерами на момент объединения их в более крупный кластер.
Пример построения:
На первом шаге у нас 5 кластеров, каждый кластер состоит из одной точки. В иерархическом кластерном анализе всегда первый шаг будет таким, вне зависимости от выбранного способа агломерации. Теперь объединим в кластер те точки, которые ближе всего друг к другу. Это точки A и B, расстояние между ними 2. Соединим эти точки, а на вертикальной оси отметим расстояние 2.
 
Нужно снова объединить точки в более крупные кластеры. Для этого необходимо определить попарные расстояния между точками C, D и E, а также расстояние между кластером A + B и точкой C, между кластером A + B и точкой D, между кластером A + B и точкой E.
Мы выбрали метод ближнего соседа, поэтому нам надо посчитать расстояние от всех точек кластера A + B до всех точек кластера E и выбрать минимальное. Как можно догадаться, при выборе метода дальнего соседа мы будем брать максимальное значение из посчитанных, а при выборе метода средней связи – среднее расстояние. Проделаем такую операцию для остальных точек и получим матрицу расстояний. 
 
Получаем три кластера: A + B, C + D, E. По той же схеме строим новые матрицы расстояний. Осталось объединить всё в один большой кластер. Финальный штрих – соединяем все ветви на расстоянии 4.2 (можете посчитать и проверить самостоятельно). Завершим построение дендрограммы!
 
Благодаря дендрограмме мы увидели все возможные варианты деления наших наблюдений на группы. Но сколько кластеров выбрать? Ответ на этот вопрос довольно простой, но при этом многозначный:
1.	стоит взять столько кластеров, сколько можем содержательно проинтерпретировать; 
2.	стоит взять столько кластеров, сколько является разумным с точки зрения выраженности межгрупповых различий.
В то время как первый критерий выбора зависит исключительно от наших экспертных, во многом субъективных, знаний, выполнение второго мы можем проверить формально, используя известные статистические методы для сравнения двух и более групп (критерий Стьюдента для двух выборок, критерий Уилкоксона, ANOVA, критерий Краскелла-Уоллиса и другие).
Наш пример с пятью наблюдениями, конечно, слишком игрушечный, чтобы всерьёз рассуждать, сколько кластеров здесь выбрать, но явно выбор будет стоять между двумя и тремя группами. Если мы хотим получить более общую классификацию и избежать слишком маленьких кластеров из одного человека, то стоит выбрать две группы: C + D и A + B + E. Если для нас выделяющиеся наблюдения играют важную роль, то деление на три кластера, где точка E обособлена, будет в приоритете.
16	Визуализация инструментов Data Mining 
Лекция: ЛЕКЦИЯ_ВИЗУАЛЬНЫЙ_АНАЛИЗ_ДАННЫХ.pdf
Каждый из алгоритмов Data Mining использует определенный подход к визуализации. В ходе использования каждого из методов, а точнее, его программной реализации, мы получали некие визуализаторы, при помощи которых нам удавалось интерпретировать результаты, полученные в результате работы соответствующих методов и алгоритмов. 
• Для деревьев решений это визуализатор дерева решений, список правил, таблица сопряженности. 
• Для нейронных сетей в зависимости от инструмента это может быть топология сети, график изменения величины ошибки, демонстрирующий процесс обучения. 
• Для карт Кохонена: карты входов, выходов, другие специфические карты. 
• Для линейной регрессии в качестве визуализатора выступает линия регрессии. 
• Для кластеризации: дендрограммы, диаграммы рассеивания. 
Диаграммы и графики рассеивания часто используются для оценки качества работы того или иного метода. Все эти способы визуального представления или отображения данных могут выполнять одну из функций: 
• являются иллюстрацией построения модели (например, представление структуры (графа) нейронной сети); 
• помогают интерпретировать полученный результат; 
• являются средством оценки качества построенной модели; 
• сочетают перечисленные выше функции (дерево решений, дендрограмма).
Визуализация Data Mining моделей
Первая функция (иллюстрация построения модели), по сути, является визуализацией Data Mining модели. Существует много различных способов представления моделей, но графическое ее представление дает пользователю максимальную "ценность".
Таким образом, доступность является одной из основных характеристик модели Data Mining. Несмотря на это, существует и такой распространенный и наиболее простой способ представления модели, как "черный ящик". В этом случае пользователь не понимает поведения той модели, которой пользуется. Однако, несмотря на непонимание, он получает результат - выявленные закономерности. Классическим примером такой модели является модель нейронной сети.
Другой способ представления модели - представление ее в интуитивном, понятном виде. В этом случае пользователь действительно может понимать то, что происходит "внутри" модели. Такие модели обеспечивают пользователю возможность обсуждать ее логику с коллегами, клиентами и другими пользователями, или объяснять ее.
Понимание модели ведет к пониманию ее содержания. В результате понимания возрастает доверие к модели. Классическим примером является дерево решений.
Кроме понимания, такие модели обеспечивают пользователя возможностью взаимодействовать с моделью, задавать ей вопросы и получать ответы. Примером такого взаимодействия является средство "что, если". При помощи диалога "система-пользователь" пользователь может получить понимание модели.
17	При помощи каких средств визуализации можно оценить качество модели? Какие помогают интерпретировать результат 
Лекция: ЛЕКЦИЯ_ВИЗУАЛЬНЫЙ_АНАЛИЗ_ДАННЫХ.pdf
Теперь перейдем к функциям, которые помогают интерпретировать и оценить результаты построения Data Mining моделей. Это всевозможные графики, диаграммы, таблицы, списки и т.д. 
Примерами средств визуализации, при помощи которых можно оценить качество модели, являются диаграмма рассеивания, таблица сопряженности, график изменения величины ошибки.
Диаграмма рассеивания представляет собой график отклонения значений, прогнозируемых при помощи модели, от реальных. Эти диаграммы используют для непрерывных величин. Визуальная оценка качества построенной модели возможна только по окончанию процесса построения модели.
Таблица сопряженности используется для оценки результатов классификации. Такие таблицы применяются для различных методов классификации. Они уже использовались нами в предыдущих лекциях. Оценка качества построенной модели возможно только по окончанию процесса построения модели. 
График изменения величины ошибки. График демонстрирует изменение величины ошибки в процессе работы модели. Например, в процессе работы нейронных сетей пользователь может наблюдать за изменением ошибки на обучающем и тестовом множествах и остановить обучение для недопущения "переобучения" сети. Здесь оценка качества модели и его изменения может оцениваться непосредственно в процессе построения модели. 
Примерами средств визуализации, которые помогают интерпретировать результат, являются: линия тренда в линейной регрессии, карты Кохонена, диаграмма рассеивания в кластерном анализе.
Из GPT:
Линия тренда в линейной регрессии - это прямая линия, которая лучше всего описывает зависимость между двумя переменными в наборе данных. В линейной регрессии линия тренда используется для прогнозирования значений зависимой переменной на основе значений независимой переменной.
Карты Кохонена (самоорганизующиеся карты) - это нейронные сети, используемые для визуализации и анализа многомерных данных. Карты Кохонена организуют данные в двумерное пространство, что позволяет выявлять скрытые структуры и паттерны. Они часто применяются в задачах кластеризации и снижения размерности данных.
Диаграмма рассеивания в кластерном анализе - это графическое представление данных, где каждая точка соответствует объекту из набора данных, а координаты точки определяются значениями двух переменных. Диаграмма рассеивания используется для визуализации распределения данных и выявления потенциальных кластеров или групп объектов. Она помогает понять, как объекты группируются в пространстве признаков и может быть полезна для оценки результатов кластерного анализа.
18	Традиционные методы визуального анализа данных 
Лекция: ЛЕКЦИЯ_ВИЗУАЛЬНЫЙ_АНАЛИЗ_ДАННЫХ.pdf
Методы визуализации Методы визуализации, в зависимости от количества используемых измерений, принято классифицировать на две группы: 
• представление данных в одном, двух и трех измерениях; 
• представление данных в четырех и более измерениях.
Представление данных в одном, двух и трех измерениях 
К этой группе методов относятся хорошо известные способы отображения информации, которые доступны для восприятия человеческим воображением. Практически любой современный инструмент Data Mining включает способы визуального представления из этой группы. В соответствии с количеством измерений представления это могут быть следующие способы: 
• одномерное (univariate) измерение, или 1-D ; 
• двумерное (bivariate) измерение, или 2-D ; 
• трехмерное или проекционное (projection) измерение, или 3-D. 
Наиболее естественно человеческий глаз воспринимает двухмерные представления информации. При использовании двух- и трехмерного представления информации пользователь имеет возможность увидеть закономерности набора данных: 
• его кластерную структуру и распределение объектов на классы (например, на диаграмме рассеивания); 
• топологические особенности; 
• наличие трендов; 
• информацию о взаимном расположении данных; 
• существование других зависимостей, присущих исследуемому набору данных. 
Если набор данных имеет более трех измерений, то возможны такие варианты:
• использование многомерных методов представления информации (они рассмотрены ниже); 
• снижение размерности до одно-, двух- или трехмерного представления. 
Существуют различные способы снижения размерности, один из них - факторный анализ (это статистический метод, используемый для выявления скрытых переменных (факторов), которые объясняют наблюдаемые корреляции между множеством исходных переменных). 
Для снижения размерности и одновременного визуального представления информации на двумерной карте используются самоорганизующиеся карты Кохонена. 
Представление данных в 4 + измерениях 
Представления информации в четырехмерном и более измерениях недоступны для человеческого восприятия. Однако разработаны специальные методы для возможности отображения и восприятия человеком такой информации. Наиболее известные способы многомерного представления информации: 
• параллельные координаты; 
• " лица Чернова "; 
• лепестковые диаграммы.
В параллельных координатах переменные кодируются по горизонтали, вертикальная линия определяет значение переменной. Пример набора данных, представленного в декартовых координатах и параллельных координатах.
Основная идея представления информации в "лицах Чернова" состоит в кодировании значений различных переменных в характеристиках или чертах человеческого лица. 
Для каждого наблюдения рисуется отдельное "лицо". На каждом "лице" относительные значения переменных представлены как формы и размеры отдельных черт лица (например, длина и ширина носа, размер глаз, размер зрачка, угол между бровями). 
Анализ информации при помощи такого способа отображения основан на способности человека интуитивно находить сходства и различия в чертах лица. 
Лепестковые диаграммы - это графическое представление многомерных данных, где каждая ось соответствует одной из переменных, и оси исходят из одной точки, образуя радиальную сетку. Значения переменных соединены линией, создавая форму (лепесток). Диаграммы позволяют сравнивать профиль данных по нескольким параметрам одновременно.
Перед использованием методов визуализации необходимо: 
• Проанализировать, следует ли изображать все данные или же какую-то их часть. 
•   Выбрать размеры, пропорции и масштаб изображения. 
• Выбрать метод, который может наиболее ярко отобразить закономерности, присущие набору данных. 
Наличие большого количества средств визуализации, представленных в инструменте, который применяет пользователь, может даже вызвать растерянность.
Среди двухмерных и трехмерных средств наиболее широко известны линейные графики, линейные, столбиковые, круговые секторные и векторные диаграммы.
При помощи линейного графика можно отобразить тенденцию, передать изменения какого-либо признака во времени. Для сравнения нескольких рядов чисел такие графики наносятся на одни и те же оси координат.
Гистограмму применяют для сравнения значений в течение некоторого периода или же соотношения величин. 
Круговые диаграммы используют, если необходимо отобразить соотношение частей и целого, т.е. для анализа состава или структуры явлений. Составные части целого изображаются секторами окружности. Секторы рекомендуют размещать по их величине: вверху - самый крупный, остальные - по движению часовой стрелки в порядке уменьшения их величины. Круговые диаграммы также применяют для отображения результатов факторного анализа, если действия всех факторов являются однонаправленными. При этом каждый фактор отображается в виде одного из секторов круга.
19	Основные тенденции в области визуализации 
Лекция: ЛЕКЦИЯ_ВИЗУАЛЬНЫЙ_АНАЛИЗ_ДАННЫХ.pdf
Как уже отмечалось, при помощи средств визуализации поддерживаются важные задачи бизнеса, среди которых - процесс принятия решений. В связи с этим возникает необходимость перехода средств визуализации на более качественный уровень, который характеризуется появлением абсолютно новых средств визуализации и взглядов на ее функции, а также развитием ряда тенденций в этой области. 
Среди основных тенденций в области визуализации Филип Рассом (Philip Russom) выделяет: 
• Разработка сложных видов диаграмм. 
• Повышение уровня взаимодействия с визуализацией пользователя.
• Увеличение размеров и сложности структур данных, представляемых визуализацией.
1.	Разработка сложных видов диаграмм. 
Большинство визуализаций данных построено на основе диаграмм стандартного типа (секторные диаграммы, графики рассеяния и.т.д.). Эти способы являются одновременно старейшими, наиболее элементарными и распространенными. В последние годы перечень видов диаграмм, поддерживаемых инструментальными средствами визуализации, существенно расширился.  
2.	Повышение уровня взаимодействия с визуализацией пользователя.
Еще совсем недавно большая часть средств визуализации представляла собой статичные диаграммы, предназначенные исключительно для просмотра. Сейчас широко используются динамические диаграммы, уже сами по себе являющиеся пользовательским интерфейсом, в котором пользователь может напрямую и интерактивно манипулировать визуализацией, подбирая новое представление информации.
Сложное взаимодействие позволяет пользователю изменять визуализацию для нахождения альтернативных интерпретаций данных. Взаимодействие с визуализацией подразумевает минимальный по своей сложности пользовательский интерфейс, в котором пользователь может управлять представлением данных, просто "кликая" на элементы визуализации, перетаскивая и помещая представления объектов данных или выбирая пункты меню. Инструменты OLAP или Data Mining превращают непосредственное взаимодействие с визуализацией в один из этапов итерационного анализа данных. Средства Text Mining или управления документами придают такому непосредственному взаимодействию характер навигационного механизма, помогающего пользователю исследовать библиотеки документов.
Визуальный запрос является наиболее современной формой сложного взаимодействия пользователя с данными. Пользователь использует информационные точки графика рассеяния, выбирать их мышкой и получать новые визуализации, представляющие именно эти точки. Приложение визуализации данных генерирует соответствующий язык запроса, управляет принятием запроса базой данных и визуально представляет результирующее множество. Пользователь может сфокусироваться на анализе, не отвлекаясь на составление запроса. Увеличение размеров и сложности структур данных, представляемых визуализацией.
Визуализация поддерживает обработку структурированных данных, она также является ключевым средством представления схем так называемых неструктурированных данных, например текстовых документов, т.е. Text Mining. В частности, средства Text Mining могут осуществлять парсинг больших пакетов документов и формировать предметные указатели понятий и тем, освещенных в этих документах. Когда предметные указатели созданы с помощью нейросетевой технологии, пользователю непросто продемонстрировать их без некоторой формы визуализации данных.  Визуализация в таком случае преследует две цели: 
•	визуальное представление контента библиотеки документов; 
•	навигационный механизм, который пользователь может применять при исследовании документов и их тем. 
Визуальный анализ данных обычно выполняется в три этапа: 
• беглый анализ - позволяет идентифицировать интересные шаблоны и сфокусироваться на одном или нескольких из них; 
• увеличение и фильтрация - идентифицированные на предыдущем этапе шаблоны отфильтровываются и рассматриваются в большем масштабе; 
• детализация по необходимости - если пользователю нужно получить дополнительную информацию, он может визуализировать более детальные данные. 

20.	Что представляет из себя логит? С какими понятиями он связан.
Логит тесно связан с понятиями «вероятность» и «шанс».
Вероятность – это объективная мера появления некоторого события, из- 
меряемая от 0 до 1. На практике оценкой вероятности служит относительная частота появления события. Значение вероятности 0 означает невозмож- ность появления события. Значение вероятности 1 означает, что событие непременно произойдет. 
Вероятность (probability): Yi 
Шансы – это отношение вероятности того, что событие произойдет, к вероят- ности того, что событие не произойдет. Можно еще сказать так: шансы – это отношение вероятности наступления события к вероятности ненаступления события. Вероятность наступления события часто называют просто веро- ятностью события, и когда вы встречаете фразы «вычислить вероятность», «оценить влияние предикторов на вероятности» в контексте логистической регрессии, то речь идет именно о вероятности события. С ростом вероятно- сти растут шансы, и наоборот. Значение шансов 1 соответствует ситуации, когда вероятности наступления события и ненаступления события равны. 
Шансы (odds):  
Наконец, логит – это натуральный логарифм шансов. 
Логит (logit), логарифм шансов (log odds), прологарифмированные шансы (logged odds):  
Поупражняемся вычислять шансы и логиты.
Например, если Pi для первого наблюдения равно 0,2, то шансы равны 0,25, 
или 0,2/0,8, а логит равен –1,386, т. е. натуральному логарифму шансов. Если Pi для второго наблюдения равно 0,7, то шансы равны 2,33, или 0,7/0,3, 
а логит равен 0,847.
Если Pi для третьего наблюдения равно 0,9, то шансы равны 9, или 0,9/0,1, 
а логит равен 2,197.
Хотя формула преобразования вероятностей в логиты проста, требуется 
некоторое объяснение, чтобы проиллюстрировать ее полезность. Оказыва- ется, она прекрасно описывает зависимость между предикторами и распре- делением вероятностей, определяемым бинарной зависимой переменной. Формула включает два шага: на первом шаге мы берем отношение вероят- ности, что событие произойдет, к вероятности, что событие не произойдет, 
 , и получаем шансы возникновения события; на втором шаге берем натуральный логарифм шансов и получаем логит. 

21.	Смысл и свойства логита.
Использование натурального логарифма шансов исключает минимальное значение 0 («пол») так же, как преобразование вероятностей в шансы ис- ключает максимальное значение 1 («потолок»). Когда мы берем: 
натуральный логарифм шансов выше 0, но ниже 1, мы получаем от- рицательные числа; 
натуральный логарифм шансов, равный 1, мы получаем 0;
натуральный логарифм шансов выше 1, мы получаем положительные 
числа.
Напомним, что логарифм 0 и отрицательных чисел не существует. 
Таким образом, первое свойство логита состоит в том, что, в отличие от вероятности, он не имеет верхней или нижней границы. Шансы устраняют верхнюю границу вероятностей, а прологарифмированные шансы устраняют нижнюю границу вероятностей. Давайте убедимся в этом. Если Pi = 1, логит не определен, потому что шансы 1/0 не существуют. По мере того как вероят- ность приближается к 1, логит движется к +¥. Если Pi = 0, логит не опреде- лен, потому что логарифм шансов 0/1 или 0 не существует. По мере того как вероятность приближается к 0, логит движется к –¥. Таким образом, логит варьирует от –¥ до +¥. Проблема нижней и верхней границ для вероятно- стей (или нижней границы для шансов) отпадает. 
Второе свойство логита заключается в том, что логит-преобразование симметрично относительно вероятности 0,5. Когда Pi = 0,5, логит равен 0 (0,5/0,5 = 1, а логарифм 1 равно 0). Вероятности ниже 0,5 (Pi меньше 1 – Pi) приведут к отрицательным логитам, потому что шансы падают ниже 1, но выше 0. А из курса школьной математики мы помним, что логарифм чисел выше 0, но ниже 1 дает отрицательное число. Вероятности выше 0,5 (Pi выше 1 – Pi) приведут к положительным логитам, потому что шансы превышают 1. Опять же из курса школьной математики помним, что логарифм чисел выше 1 дает положительное число. 
Кроме того, вероятности, которые находятся на одинаковом расстоянии от 0,5 выше или ниже (например, 0,6 и 0,4, 0,7 и 0,3, 0,8 и 0,2), имеют одина- ковые логиты, но с разными знаками (например, логиты для вероятностей, перечисленных выше, равны 0,405 и –0,405, –0,847 и –0,847, 1,386 и –1,386). Удаленность логита от 0 отражает удаленность вероятности от 0,5 (опять же отметим, что логиты не имеют границ). 
Третье свойство логита заключается в том, что одно и то же изменение вероятности приводит к различным изменениям в логитах. Простой прин- цип заключается в том, что по мере приближения Pi к 0 и 1 одно и то же изменение вероятности приводит к большему изменению логита. Вы можете увидеть это на примере:
 
Изменение вероятности на 0,1 с 0,5 до 0,6 (или с 0,5 до 0,4) приводит к из- менению логита на 0,405, тогда как такое же изменение вероятности на 0,1 с 0,8 до 0,9 (или с 0,2 до 0,1) приводит к изменению логита на 0,81. 
Для одного и того же изменения вероятности изменение логита в край- них значениях вероятности будет в два раза больше изменения логита для среднего значения вероятности. Повторим, что общий принцип заключается в том, что небольшое изменение вероятности приводит к большому изменению логита, когда вероятности находятся вблизи границ 0 и 1.  

22.	Линейная модель множественной регрессии, спецификация модели. Требования к факторам. Коэффициент корреляции.
Парная регрессия может дать хороший результат при моделировании, если влиянием других факторов, воздействующих на объект исследования, можно пренебречь. Если же этим влиянием пренебречь нельзя, то следует попытаться выявить влияние других факторов, введя их в модель, т.е. построить уравнение множественной регрессии: 
 
где y – зависимая переменная (результативный признак), i x – независимые, или объясняющие, переменные (признаки-факторы). Множественная регрессия широко используется в решении проблем спроса, доходности акций, при изучении функции издержек производства, в макроэкономических расчетах и целом ряде других вопросов эконометрики. В настоящее время множественная регрессия – один из наиболее распространенных методов в эконометрике. Основная цель множественной регрессии – построить модель с большим числом факторов, определив при этом влияние каждого из них в отдельности, а также совокупное их воздействие на моделируемый показатель.
 
Построение уравнения множественной регрессии начинается с решения вопроса о спецификации модели. Он включает в себя два круга вопросов: отбор факторов и выбор вида уравнения регрессии.
Включение в уравнение множественной регрессии того или иного набора факторов связано прежде всего с представлением исследователя о природе взаимосвязи моделируемого показателя с другими экономическими явлениями. Факторы, включаемые во множественную регрессию, должны отвечать следующим требованиям: 
1. Они должны быть количественно измеримы. Если необходимо включить в модель качественный фактор, не имеющий количественного измерения, то ему нужно придать количественную определенность. 
2. Факторы не должны быть интеркоррелированы и тем более находиться в точной функциональной связи. Включение в модель факторов с высокой интеркорреляцией, может привести к нежелательным последствиям – система нормальных уравнений может оказаться плохо обусловленной и повлечь за собой неустойчивость и ненадежность оценок коэффициентов регрессии. Если между факторами существует высокая корреляция, то нельзя определить их изолированное влияние на результативный показатель и параметры уравнения регрессии оказываются неинтерпретируемыми.

 Корреляция – вероятностная связь, взаимозависимость случайных величин. • Коэффициент корреляции — это показатель степени связи между двумя переменными или измерениями.

23.	Показатели качества множественной регрессии (оценка значимости коэффициентов регрессии, F-критерий, Коэффициент детерминации, Частные коэффициенты корреляции.
 
 
 

Оценка значимости коэффициентов регрессии позволяет определить, насколько каждая независимая переменная влияет на зависимую переменную. Это важно для понимания структуры модели и выявления наиболее значимых факторов. Основным инструментом для оценки значимости коэффициентов регрессии является t-критерий. 
 
24.	Этапы кластерного анализа. Многомерные данные.
Кластерный анализ предназначен для разбиения исходных данных на поддающиеся интерпретации группы, таким образом, чтобы элементы, входящие в одну группу были максимально «схожи», а элементы из разных групп были максимально «отличными» друг от друга.
 
 
Многомерные данные представляют собой набор данных, в котором каждый объект описывается несколькими признаками (переменными). Важно учитывать корреляции между признаками и возможную избыточность данных. 
25.	Цели и целевые функции. Значимость переменных объекта.
Цели и целевые функции — это ключевые элементы при построении и оценке моделей в анализе данных. Они определяют, что модель должна достигнуть и каким образом будет измеряться её эффективность. 
Цели анализа данных варьируются в зависимости от конкретной задачи и области применения, но могут включать:
- Предсказание
 Построение модели для прогнозирования будущих значений зависимой переменной на основе новых данных.
- Классификация
Разделение данных на категории или классы.
- Кластеризация
Группировка объектов в кластеры на основе их сходства.
Целевая функция (или функция потерь) — это математическое выражение, которое оценивает качество модели. Она используется для обучения модели и минимизации ошибок. Выбор целевой функции зависит от типа задачи:
Регрессия: 
Основные целевые функции включают среднеквадратичную ошибку (MSE), среднюю абсолютную ошибку (MAE). 
Классификация:
 Используются логистическая регрессия, перекрестная энтропия и AUC-ROC.

Значимость переменных (факторов) — это оценка вклада каждой независимой переменной в объяснение вариации зависимой переменной. Определение значимости переменных позволяет выявить ключевые факторы, влияющие на целевую переменную, и упростить модель, исключив незначимые переменные.



26.	Этапы кластерного анализа. Данные о близости.
 
Данные о близости (или данные о расстояниях) являются ключевым компонентом кластерного анализа. Они отражают степень сходства или различия между объектами в многомерном пространстве. Рассмотрим основные аспекты данных о близости:
Метрики расстояний используются для количественной оценки близости между объектами. Выбор метрики зависит от типа данных и специфики задачи.
 
Для того, чтобы реализовать иерархический кластерный анализ, нам необходимо опреде литься с метрикой и получить матрицу расстояний – матрицу, состоящую из расстояний между всеми парами точек. Как можно догадаться, эта матрица будет квадратной, симмет- ричной, а на главной диагонали будут находиться 0 (вспомним свойства метрики). 
27 Переменные объекта и факторы. 
Наблюдаемая переменная — переменная, которую зафиксировали в явном виде.
 Скрытая переменная — переменная, которую вывели через математические модели с использованием наблюдаемых переменных.
Факторный анализ – процедура, с помощью которой большое число переменных, относящихся к имеющимся наблюдениям, сводят к меньшему количеству независимых влияющих величин, называемых факторами:
- в один фактор объединяются переменные, сильно коррелирующие между собой
- переменные из разных факторов слабо коррелируют между собой.
Факторный анализ классифицирует признаки (переменные), описывающие наблюдения.
Фактор – латентная (скрытая) переменная, конструируемая таким образом, чтобы можно было объяснить корреляцию между набором имеющихся переменных. Концепция факторного анализа заключается в сжатии информации. 
28 Проверка достоверности результатов.
Проверить достоверность результатов можно с помощью метрик recall, presision, accuracy, матрицы ошибок, критерия Фишера, критерия Стьюдента, ROC-кривой.
recall – полнота модели - доля истинно положительных, среди всех действительно положительных
presision – точность, доля истинно положительных, среди всех, которые модель классифицировала как положительные
accuracy - насколько правильно модель классифицировала данные. Это отношение числа правильно классифицированных экземпляров к общему числу экземпляров.
ROC-кривая – используется для оценки качества классификационной модели. Она показывает, насколько хорошо модель различает два класса, и помогает выбрать оптимальный порог вероятности для классификации.
	Матрица ошибок – нужна для оценки точности в задачах классификации. Это таблица с 4 различными комбинациями прогнозируемых и фактических значений. Прогнозируемые значения описываются как положительные и отрицательные, а фактические – как истинные и ложные. 
	Критерий Фишера — статистический критерий для оценки значимости различия дисперсий двух случайных выборок. С помощью Критерия Фишера оценивают качество регрессионной модели. Оценивание качества уравнения регрессии - состоит в проверке гипотезы Но о статистической незначимости уравнения регрессии и показателя тесноты связи. Для этого выполняется сравнение фактического Fфакт и критического (табличного) Fтабл значений F-критерия Фишера. Fфакт определяется из соотношения значений факторной и остаточной дисперсий, рассчитанных на одну степень свободы. Fтабл - это максимально возможное значение критерия под влиянием случайных факторов при данных степенях свободы и уровне значимости a. Уровень значимости a - вероятность отвергнуть правильную гипотезу при условии, что она верна. Обычно a принимается равной 0,05 или 0,01. Если Fтабл < Fфакт, то Но - гипотеза о случайной природе оцениваемых характеристик отклоняется и признается их статистическая значимость и надежность. Если Fтабл > Fфакт, то гипотеза Но не отклоняется и признается статистическая незначимость, ненадежность уравнения регрессии.
	Критерий Стьюдента – Для оценки статистической значимости коэффициентов регрессии и корреляции рассчитываются t-критерий Стьюдента и доверительные интервалы каждого из показателей. Выдвигается гипотеза Но о случайной природе показателей, т.е. о незначимом их отличии от нуля. Оценка значимости коэффициентов регрессии и корреляции с помощью t-критерия Стьюдента проводится путем сопоставления их значений с величиной случайной ошибки. Сравнивая фактическое и критическое (табличное) значения t-статистики - tтабл и tфакт - принимаем или отвергаем гипотезу Но.

29 Регрессионный анализ. Основная идея. Линейность и нелинейность по параметрам и факторам. Линеаризация.
Регрессионный анализ - это статистический метод выявления и количественной оценки связи между зависимой переменной и одной или несколькими независимыми переменными. Уравнение регрессии показывает, как в среднем изменяется у при изменении любого из xi.
Если независимая переменная одна - это простой регрессионный анализ. Если же их несколько, то такой анализ называется многофакторным.
В ходе регрессионного анализа решаются две основные задачи:
	построение уравнения регрессии, т.е. нахождение вида зависимости между результатным показателем и независимыми факторами x1, x2, …, xn.
	оценка значимости полученного уравнения, т.е. определение того, насколько выбранные факторные признаки объясняют вариацию признака у.
Нелинейная по переменным модель - линейная модель y=f(x), в которой возможна замена переменной  z=g(x), приводящая получившуюся модель у = F(z) - к линейной.
Нелинейная по параметрам модель - модель, которую нельзя привести заменами переменных к линейной.
Линейная регрессия: y = a + bx + ε
Нелинейные регрессии делятся на два класса: регрессии, нелинейные относительно включенных в анализ объясняющих переменных, но линейные по оцениваемым параметрам, и регрессии, нелинейные по оцениваемым параметрам.
Регрессии, нелинейные по объясняющим переменным:
•	полиномы разных степеней y=a+b1·x+b2·x2+b3·x3+ε
•	равносторонняя гипербола  .
Регрессии, нелинейные по оцениваемым параметрам:
•	степенная y=a·xb·ε
•	показательная y=a·bx·ε
•	экспоненциальная y=ea+b·x·ε
Линеаризация – подбор преобразований к анализируемым переменным, которые позволили бы представить искомую зависимость в виде линейного соотношения между преобразованными переменными. 

30 Переменные объекта, их значения и взаимосвязи.
Переменные объекта в анализе данных - это характеристики или атрибуты, которые мы изучаем. Они могут быть различными и зависят от конкретного исследования. Например, если мы анализируем данные о погоде, переменными объекта могут быть температура, влажность, скорость ветра и так далее.
Значения переменных - это конкретные измерения или наблюдения, которые мы собираем для каждой переменной. Например, если температура является одной из наших переменных, то конкретные температурные показатели, которые мы записываем, будут являться значениями этой переменной.
Взаимосвязи между переменными - это ключевой аспект анализа данных. Мы хотим понять, как одна переменная влияет на другую. Например, мы можем исследовать, как температура влияет на влажность. Это может быть выражено с помощью статистических методов, таких как корреляционный анализ или регрессионный анализ.
В контексте многомерного анализа, мы можем изучать взаимосвязи между несколькими переменными одновременно. Это может помочь нам понять сложные взаимодействия и закономерности в данных.
Важно отметить, что анализ данных - это итеративный процесс. Мы можем начать с определенного набора переменных и гипотез, а затем по мере получения новых данных и информации мы можем обновлять наши переменные и гипотезы. Это помогает нам постоянно улучшать наше понимание данных и делать более точные прогнозы и выводы.
31 Метод DBSCAN.
DBSCAN – алгоритм кластеризации, развивает идею кластеризации с помощью выделения связных компонент.
Плотность в DBSCAN определяется в окрестности каждого объекта выборки 𝑥𝑖 как количество других точек выборки в шаре 𝐵(𝜀,𝑥𝑖). Кроме радиуса 𝜀 окрестности в качестве гиперпараметра алгоритма задается порог 𝑁0 по количеству точек в окрестности.
Далее все объекты выборки делятся на три типа: внутренние / основные точки (core points), граничные (border points) и шумовые точки (noise points). К основным относятся точки, в окрестности которых больше 𝑁0 объектов выборки. К граничным — точки, в окрестности которых есть основные, но общее количество точек в окрестности меньше 𝑁0. Шумовыми называют точки, в окрестности которых нет основных точек и в целом содержится менее 𝑁0 объектов выборки.
Алгоритм кластеризации с помощью DBSCAN выглядит следующим образом:
1.	Шумовые точки убираются из рассмотрения и не приписываются ни к какому кластеру.
2.	Основные точки, у которых есть общая окрестность, соединяются ребром.
3.	В полученном графе выделяются компоненты связности.
4.	Каждая граничная точка относится к тому кластеру, в который попала ближайшая к ней основная точка.
Удобство DBSCAN заключается в том, что он сам определяет количество кластеров (по модулю задания других гиперпараметров — 𝜀 и 𝑁0), а также в том, что метод успешно справляется даже с достаточно сложными формами кластеров. Кластеры могут иметь вид протяжённых лент или быть вложенными друг в друга как концентрические гиперсферы. На изображении ниже показан пример выделения кластеров достаточно сложной формы с помощью DBSCAN:
 
DBSCAN — один из самых сильных алгоритмов кластеризации, но работает он, как правило, заметно долго, к тому же весьма чувствителен к размерности пространства признаков, поэтому используется на практике DBSCAN только тогда, когда успевает отрабатывать за приемлемое время.

32 Общие предположения о выборке и процедуре проведения эксперимента
 
Пусть x1,x2,..,xn − выборка объема n из некоторой генеральной совокупности. По этой выборке можно оценить основные числовые характеристики генеральной совокупности. Различные элементы выборки i x называются вариантами.
Выборка называется случайной, если из генеральной совокупности элементы берутся наугад и в выборку каждый из них может попасть с одинаковой вероятностью. 
Если случайная выборка такова, что по её распределению по некоторому признаку можно судить о распределении поэтому же признаку неизвестной генеральной совокупности, то такая выборка называется репрезентативной, т.е. хорошо представляющей генеральную совокупность.
Статистическая гипотеза — это определённое предположение о распределении вероятностей, лежащем в основе наблюдаемой выборки данных.
Проверка статистической гипотезы — это процесс принятия решения о том, противоречит ли рассматриваемая статистическая гипотеза наблюдаемой выборке данных.
В анализе больших данных эксперимент проводится, чтобы подтвердить или опровергнуть гипотезы. 
Процедура проведения эксперимента:

 
 
Более простое объяснение:
Основные этапы проверки статистических гипотез
1.	Исходя из задач исследования, формулируются статистические гипотезы
2.	Выбирается уровень значимости, на котором будут проверяться гипотезы
3.	На основе выборки, полученной из результатов измерения, определяется статистическая характеристика гипотезы
4.	Выбирается критерий для проверки статистической гипотезы
5.	Вычисляется наблюдаемое (фактическое) значение статистического критерия
6.	Определяется критическое значение статистического критерия по соответствующей таблице на основании выбранного уровня значимости и объема выборки
7.	На основе сравнения наблюдаемого и критического значения критерия в зависимости от результатов проверки нулевая гипотеза либо принимается, либо отклоняется в пользу альтернативной.
33. Типы классификаций. Кластер. Типы кластерных структур.
Классификация - системное распределение изучаемых предметов, явлений, процессов по родам, видам, типам, по каким-либо существенным признакам для удобства их исследования; группировка исходных понятий и расположение их в определенном порядке, отражающем степень этого сходства.
Классификация - упорядоченное по некоторому принципу множество объектов, которые имеют сходные классификационные признаки (одно или несколько свойств), выбранных для определения сходства или различия между этими объектами.
Классификация относится к стратегии обучения с учителем (supervised learning), которое также именуют контролируемым или управляемым обучением.
Классификация	может	быть	одномерной	(по	одному	признаку)	и
многомерной (по двум и более признакам).
Различают:
•	вспомогательную (искусственную) классификацию, которая производится по внешнему признаку и служит для придания множеству предметов (процессов, явлений) нужного порядка;
•	естественную классификацию, которая производится по существенным  признакам,  характеризующим  внутреннюю  общностьпредметов и явлений. Она является результатом и важным средством научного исследования, т.к. предполагает и закрепляет результаты изучения закономерностей классифицируемых объектов.
В зависимости от выбранных признаков, их сочетания и процедуры деления понятий классификация может быть:
•	простой - деление родового понятия только по признаку и только один раз до раскрытия всех видов. Примером такой классификации является дихотомия, при которой членами деления бывают только два понятия, каждое из которых является противоречащим другому (т.е. соблюдается принцип: "А и не А");
•	сложной - применяется для деления одного понятия по разным основаниям и синтеза таких простых делений в единое целое. Примером такой классификации является периодическая система химических элементов.
Под классификацией будем понимать отнесение объектов (наблюдений, событий) к одному из заранее известных классов.
Кластеризация является описательной процедурой, она не делает никаких статистических выводов, но дает возможность провести разведочный анализ и изучить "структуру данных".
Само понятие "кластер" определено неоднозначно: в каждом исследовании свои "кластеры". Переводится понятие кластер (cluster) как "скопление", "гроздь".
Кластер можно охарактеризовать как группу объектов, имеющих общие свойства. Характеристиками кластера можно назвать два признака:
•	внутренняя однородность;
•	внешняя изолированность.

Приведем краткую характеристику подходов к кластеризации.

•	Алгоритмы, основанные на разделении данных (Partitioning algorithms), в т.ч. итеративные:
◦	разделение объектов на k кластеров;
◦	итеративное	перераспределение	объектов	для	улучшения кластеризации.
•	Иерархические алгоритмы (Hierarchy algorithms):
◦	агломерация:	каждый	объект	первоначально	является	кластером, кластеры, соединяясь друг с другом, формируют больший кластер и т.д.
•	Методы,	основанные	на	концентрации	объектов	(Density-based methods):
◦	основаны на возможности соединения объектов;
◦	игнорируют шумы, нахождение кластеров произвольной формы.
•	Грид-методы (Grid-based methods):
◦	квантование объектов в грид-структуры.
•	Модельные методы (Model-based):
◦	использование	модели	для	нахождения	кластеров,	наиболее соответствующих данным.

 
34. Представление исходных данных в регрессионном анализе.
1. Сбор данных, которые должны быть релевантны исследуемому вопросу. Данные могут быть собраны из различных источников, включая эксперименты, опросы.
2. Исходные данные состоят из двух основных типов переменных:
	Зависимая переменная (целевая переменная) – переменная, значение которой мы пытаемся предсказать или объяснить через модель;
	Независимые переменные (предикторы) – переменные, которые используются для предсказания значения зависимой переменной. 
3. Формат данных
Данные должны быть организованы в формате, пригодном для анализа. Обычно это таблица, где строки представляют наблюдения, а столбцы — переменные.
4. Предварительная обработка данных
Перед проведением анализа данные часто требуют предварительной обработки:
	Очистка данных. Удаление или корректировка неправильных, пропущенных или аномальных значений;
	Преобразование данных. Нормализация данных для улучшения их распределения или масштабирования.
5. Набор исходных данных (или выборку данных) разбивают на два множества: 
	Обучающее множество (training set) - множество, которое включает данные, использующиеся для обучения (конструирования) модели. Такое множество содержит входные и выходные (целевые) значения примеров. Выходные значения предназначены для обучения модели.
	Тестовое (test set) множество также содержит входные и выходные значения примеров. Здесь выходные значения используются для проверки работоспособности модели.


35. Задачи классификации. Общая постановка.
Задачей классификации часто называют предсказание категориальной зависимой переменной (т.е. зависимой переменной, являющейся категорией) на основе выборки непрерывных и/или категориальных переменных.

Например, можно предсказать, кто из клиентов фирмы является потенциальным покупателем определенного товара, а кто - нет, кто воспользуется услугой фирмы, а кто - нет, и т.д. Этот тип задач относится к задачам бинарной классификации, в них зависимая переменная может принимать только два значения (например, да или нет, 0 или 1).

Другой вариант классификации возникает, если зависимая переменная может принимать значения из некоторого множества предопределенных классов. Например, когда необходимо предсказать, какую марку автомобиля захочет купить клиент. В этих случаях рассматривается множество классов для зависимой переменной.
 
36. Причины неадекватности уравнения регрессии. Классическая схема линейного регрессионного анализа.
Причины неадекватности уравнения регрессии
Метод наименьших квадратов (МНК) — это стандартный статистический метод для оценки параметров в линейной регрессии. Этот метод стремится минимизировать сумму квадратов разностей между наблюдаемыми значениями зависимой переменной и значениями, предсказанными моделью регрессии. Для адекватной работы этого метода и обеспечения точности, надежности и статистической значимости результатов регрессионного анализа должны выполняться определенные предпосылки.

 
Теорема Гаусса - Маркова
Теорема. Если предпосылки 1 – 5 выполнены, то оценки, полученные по МНК, обладают следующими свойствами:

 1. Оценки являются несмещенными. Это говорит об отсутствии систематической ошибки при определении положения линии регрессии.

 2. Оценки состоятельны. Это означает, что с ростомnнадежность оценок возрастает.

 3. Оценки эффективны, т.е. они имеют наименьшую дисперсию по сравнению с любыми другими оценками данных параметров, линейными относительно величин yi.

 
Рис.-  Зависимость случайных остатков от величины фактора xj
Если расположение остатков на графике не имеет направленности, то они независимы от значений xj (см. рис). Если же график показывает наличие зависимости i и xj, то модель неадекватна. Причины неадекватности могут быть разные. Возможно, нарушена третья предпосылка МНК и дисперсия остатков непостоянна для каждого значения фактора xj. Может быть неправильной спецификация модели, и в нее необходимо ввести дополнительные члены от xj, например, xj2, или преобразовать значения y. Скопление точек в определенных участках значений фактора xi говорит о наличии систематической погрешности модели.

Классическая схема линейного регрессионного анализа
1. Определение модели
 
2. Сбор данных
3. Оценка параметров, используя метод наименьших квадратов (МНК)
4. Проверка адекватности модели
После оценки параметров проводится анализ остатков (разниц между наблюдаемыми и модельными значениями), чтобы проверить, выполняются ли предпосылки МНК (гомоскедастичность, нормальность распределения ошибок, отсутствие автокорреляции). Также проводятся различные статистические тесты, включая тесты на значимость коэффициентов (t-тесты) и на адекватность модели в целом (F-тест).
5. Интерпретация результатов
На этом этапе анализируются и интерпретируются полученные коэффициенты. Оценивается, как изменение независимых переменных влияет на зависимую переменную.
6. Применение модели
Финальный шаг — использование регрессионной модели для прогнозирования значений зависимой переменной или для понимания взаимосвязей между переменными.
 
37. МНК. Основная идея. Диаграмма рассеяния.

Метод наименьших квадратов (МНК) — математический метод, применяемый для решения различных задач. Он основан на минимизации суммы квадратов отклонений некоторых функций от экспериментальных входных данных.

Примеры применения МНК:
Решение переопределённых систем уравнений (когда количество уравнений превышает количество неизвестных).
Поиск решения в случае обычных (не переопределённых) нелинейных систем уравнений.
Аппроксимация точечных значений некоторой функции.
МНК является одним из базовых методов регрессионного анализа для оценки неизвестных параметров регрессионных моделей по выборочным данным.

Из лекции:
Диаграмма рассеяния, представляющая собой зависимость между непрерывной независимой переменной и бинарной зависимой переменной
Из интернета: Диаграмма рассеяния (также точечная диаграмма, англ. scatter plot) — математическая диаграмма, изображающая значения двух переменных в виде точек на декартовой плоскости. Могут использоваться и полярные координаты, особенно в случаях, когда одна из переменных представляет собой физическое направление или имеет циклический характер.  



38. Меры типа расстояния.
Из лекции:
 
Из гпт:
 
 
 
39. Анализ остатков. Построение графиков
Из лекции:
Проверка на наличие гетероскедастичности.
1)	Методом графического анализа остатков. В этом случае по оси абсцисс откладываются значения объясняющей переменной Xi, а по оси ординат квадраты отклонения εi 2 . Если имеется определенная связь между отклонениями, то гетероскедастичность имеет место. Отсутствие зависимости скоре всего будет свидетельствовать об отсутствии гетероскедастичности
2)	При помощи теста ранговой корреляции Спирмена. https://math.semestr.ru/corel/spirmen.php
Задаются два параметра: xi, ε.
 

Из интернета:
Анализ остатков— это инструмент для оценки соответствия статистической модели. Он позволяет изучить разницу между прогнозируемыми значениями и фактическими значениями переменной отклика. 
Анализ остатков помогает определить, правильно ли модель определяет взаимосвязь между предикторами и переменной ответа.

Основные характеристики остатков:
1.	Независимость. Если остатки не являются независимыми, это указывает на то, что в данных есть шаблон, который модель не захватила.
2.	Распределённость. Если остатки обычно не распределены, это может указывать на то, что модель не подходит для данных.
3.	Постоянная дисперсия. Если дисперсия остатков изменяется со значениями прогнозируемой переменной, это указывает на то, что модель не подходит для данных.

 
40. Общии ̆ подход к оцениванию факторов. Общая постановка задачи.
Из лекции:
При оценке мультиколлинеарности факторов следует учитывать, что чем ближе к 0 определитель матрицы межфакторной корреляции, тем сильнее мультиколлинеарность факторов и ненадежнее результаты множественной регрессии.
Для отбора наиболее значимых факторов xi учитываются следующие условия:
-	связь между результативным признаком и факторным должна быть выше межфакторной связи;
-	связь между факторами должна быть не более 0.7;
-	при высокой межфакторной связи признака отбираются факторы с меньшим коэффициентом корреляции между ними.
Более объективную характеристику тесноты связи дают частные коэффициенты корреляции, измеряющие влияние на результат фактора xi при неизменном уровне других факторов.

Из гпт:

1.	Постановка задачи:
Определение цели анализа: Необходимо четко сформулировать, какую информацию вы хотите извлечь из данных и для каких целей. Это может быть предсказание будущих событий, выявление закономерностей, улучшение бизнес-процессов и т.д.
Определение факторов и переменных: Необходимо определить, какие именно факторы (переменные) будут учитываться в анализе. Факторы могут быть внутренними (например, данные о продажах) и внешними (например, экономические индикаторы).
2.	Сбор данных:
Источники данных: Определение источников данных (базы данных, веб- сайты, сенсоры и т.д.) и методы их получения.
Очистка данных: Обработка и очистка данных для удаления ошибок, дубликатов и пропусков.
3.	Предварительный анализ данных:
Описание данных: Исследование структуры данных, анализ распределения значений переменных.
Корреляционный анализ: Выявление взаимосвязей между различными переменными.
4.	Выбор модели и методов:
Методы статистического анализа: Линейная регрессия, логистическая регрессия, ANOVA и т.д.
Методы машинного обучения: Решающие деревья, случайные леса, градиентный бустинг, нейронные сети и т.д.
5.	Обучение модели:
Разделение данных: Разделение данных на тренировочную и тестовую выборки.
Обучение и валидация: Обучение модели на тренировочной выборке и оценка ее качества на валидационной выборке.

6.	Оценка значимости факторов:
Статистические методы: t-тесты, p-значения, коэффициенты корреляции и другие методы для оценки значимости переменных.

Методы машинного обучения: Оценка важности признаков с помощью методов вроде Gini Importance в решающих деревьях или методов регуляризации в линейных моделях.

7.	Интерпретация и визуализация результатов:

Интерпретация: Оценка полученных результатов с точки зрения их практической значимости.

Визуализация: Использование графиков, диаграмм и других методов визуализации для наглядного представления результатов.

8.	Внедрение и мониторинг:

Внедрение модели: Интеграция модели в бизнес-процессы или ИТ-системы.
Мониторинг и обновление: Постоянный мониторинг работы модели и обновление ее по мере необходимости.
 
41. МНК. Функция потерь. Система нормальных уравнений. 

Из гпт:
МНК (Метод Наименьших Квадратов)
1.	Основные концепции:

Метод наименьших квадратов (МНК) используется для оценки параметров линейной регрессии, минимизируя сумму квадратов разностей между наблюдаемыми значениями и предсказанными значениями, полученными моделью.
 
2.	ФУНКЦИЯ ПОТЕРЬ:

Функция потерь для МНК определяется как сумма квадратов ошибок (разностей между наблюдаемыми значениями и предсказанными значениями):  
3.	СИСТЕМА НОРМАЛЬНЫХ УРАВНЕНИЙ:

Для минимизации функции потерь используется частичное дифференцирование по каждому параметру βj и приравнивание производных к нулю. Это приводит к системе нормальных уравнений:

4.	Решение системы нормальных уравнений:

Для получения оценки параметров β необходимо решить систему нормальных уравнений. Это можно сделать путем умножения обеих частей уравнения на (X ^T X) ^−1:
 

 
42. Вычисление оценки дисперсии ошибки измерения целевой функции

Лекция: Уравнение_множественной_регрессии.pdf Гомоскедастичность (постоянство дисперсий отклонений). Дисперсия случайных отклонений εi постоянна: D(εi) = D(εj) = S2 для любых i и j.
 
Найдем парные коэффициенты корреляции. 
   
   
Дисперсии параметров модели определяются соотношением S 2i = Kii, т.е. это элементы, лежащие на главной диагонали
GPT
Вычисление оценки дисперсии ошибки измерения целевой функции — важный этап анализа данных, особенно в задачах машинного обучения и статистики.
 
 


43. Регрессионный анализ. Основная идея. Регрессионная, функциональная, аппроксимирующая	зависимости.	Зависимые	и независимые переменные.
 
GPT
Регрессионный анализ — набор статистических методов исследования влияния одной или нескольких независимых переменных на зависимую переменную.
Цели регрессионного анализа:
•	Определение степени детерминированности вариации критериальной (зависимой) переменной предикторами (независимыми переменными).
•	Предсказание значения зависимой переменной с помощью независимой переменной.
•	Определение вклада отдельных независимых переменных в вариацию зависимой.
•	Наиболее распространённый вид регрессионного анализа — линейная регрессия, когда находят линейную функцию, которая, согласно определённым математическим критериям, наиболее соответствует данным.
Регрессионная зависимость характеризует среднюю тенденцию изменения зависимой переменной при изменении независимых переменных. Эта зависимость выражается через регрессионную модель, обычно в виде уравнения. Например, в линейной регрессии эта зависимость выражается уравнением прямой линии.
Функциональная зависимость — это строгая математическая зависимость, при которой каждому значению независимой переменной соответствует одно определенное значение зависимой переменной. В регрессионном анализе функциональная зависимость редко встречается, так как реальные данные часто содержат случайные колебания.
Аппроксимирующая зависимость используется для описания реальных данных, которые могут быть шумными и содержать ошибки. Цель аппроксимации — найти гладкую функцию, которая наилучшим образом приближает данные. Регрессионные модели часто используются в качестве аппроксимирующих зависимостей.
Зависимая переменная — это переменная, которую мы пытаемся предсказать или объяснить. Независимые переменные — это переменные, которые используются для объяснения вариации зависимой переменной 
44	Метод k-средних.

Из лекции:
Задача классификации решается при помощи различных методов, наиболее простой - линейная регрессия. Выбор метода должен базироваться на исследовании исходного набора данных. Наиболее распространенные методы решения задачи кластеризации: метод k-средних (работает только с числовыми атрибутами), иерархический кластерный анализ (работает также с символьными атрибутами), метод SOM. Сложностью кластеризации является необходимость ее оценки.

Если речь идёт об иерархическом кластерном анализе, заранее знать количество кластеров, которое мы хотим получить, необязательно, однако если мы говорим о кластеризации методом k-средних, количество желаемых кластеров является необходимым параметром. О различиях этих видов кластерного анализа мы поговорим чуть позже, пока стоит отметить, что в любом случае априорная информация о количестве кластеров будет полезна.

Из гпт:
Метод k-средних — это алгоритм кластеризации, который стремится разделить набор данных на 
𝑘
k предварительно заданных групп так, чтобы каждая точка данных принадлежала кластеру с ближайшим средним значением. Он широко используется в анализе данных для выявления групп или кластеров схожих элементов в наборе данных. Вот подробное объяснение каждого шага алгоритма:
 
 
 
45	Расчет коэффициентов регрессии.
 
  
 

 
46	Информационная матрица. Ее свойства. Матрица ошибок.
Информационная матрица Фишера используется для вычисления ковариационных матриц, связанных с оценками максимального правдоподобия.
В математической статистике информация Фишера (иногда называемая просто информацией) - это способ измерения количества информации, которую наблюдаемая случайная величина X несет в себе о неизвестном параметре θ распределения, моделирующего X. Формально это дисперсия оценки или ожидаемое значение наблюдаемой информации.

 
Свойства:
	Симметричность: Информационная матрица симметрична
	Положительная полуопределенность: Информационная матрица является положительно полуопределённой, что означает, что все её собственные значения неотрицательны.
	 
Матрица ошибок (неточностей) — это инструмент, который позволяет определить, в чём модель ошибается.
Эта матрица сравнивает количество правильных и неправильных предсказаний для каждого класса.
	В матрицу ошибок записывают 4 вида результатов:
	Истинно положительные (ИП/TP): количество положительных наблюдений, которые модель правильно предсказала как положительные.

	Ложноположительные (ЛП/FP): количество отрицательных наблюдений, которые модель неверно предсказала как положительные.
	Истинно отрицательные (ИО/TN): количество отрицательных наблюдений, которые модель правильно предсказала как отрицательные.
	Ложноотрицательные (ЛО/FN): количество положительных наблюдений, которые модель неверно предсказала как отрицательные.
 



